\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsthm}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{float}
% graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder

 \hypersetup{
     colorlinks=true,
     linkcolor=blue,
     filecolor=blue,
     citecolor = black,
     urlcolor=cyan,
     }


\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[section]

\setlength{\parindent}{2em}

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }}

% Update your Headers here
\fancyhead[LO]{UGP, 21-22 Even Semester}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}




%% Title
\title{Updates - MITACS project
%%%% Cite as
%%%% Update your official citation here when published
%\thanks{\textit{\underline{Citation}}:
%\textbf{Authors. Title. Pages.... DOI:000000/11111.}}
}

% \author{
%   Vivek Kumar Singh \\
%   Bachelor of Science \\
%   Department of Mathematics and Statistics \\
%   Indian Institute of Technology Kanpur \\
%   % City\\
%   \texttt{\{vksingh\}@iitk.ac.in} \\
%   %% examples of more authors
%    \And
%   Dootika Vats \\
%   Associate Professor \\
%   Department of Mathematics and Statistics \\
%   Indian Institute of Technology Kanpur \\
%  % City\\
%  \texttt{\{dootika\}@iitk.ac.in}
%   %% \AND
%   %% Coauthor \\
%   %% Affiliation \\
%   %% Address \\
%   %% \texttt{email} \\
%   %% \And
%   %% Coauthor \\
%   %% Affiliation \\
%   %% Address \\
%   %% \texttt{email} \\
%   %% \And
%   %% Coauthor \\
%   %% Affiliation \\
%   %% Address \\
%   %% \texttt{email} \\
% }


\begin{document}
\maketitle
	\section{Week 1}
		\subsection{Deciding on the project}
			We talked about three project ideas
			\begin{enumerate}
				\item Explicit block couplings on "symmetric", "big" $\Omega$'s.\\
					Eg. The Ising model on $\mathbb{Z}^2_n$
				\item Existence of MMC's
					Look at prof Aaron's notes on MMC.
				\item Computing couplings for stats. (Eg. work done by Jacob Pierre)\\
					Read: Pierre's notes on Couplings
			\end{enumerate}
			I decided to try the third one first.

		\subsection{To proceed on the third project}
			There were three tasks to be done.
			\begin{enumerate}
				\item Set up dropbox to write notes\\
					\Update: Done.
				\item Find Pierre's notes on Couplings and read a bit.\\
					\Update: I found the notes. I'm reading them up, and will be done with 3 chapters of it by our next meeting (I guess on Tuesday)
				\item Try a simple example (Suggestion : MH algoritm on $N(0, 1)$ with mirror and maximal)\\

			\end{enumerate}

	\section{Week 2}
		\begin{itemize}
			\item Created a github repo for the whole project.
			\item Coded up MH on N(0, 1) using the maximal couplings. \\
			The algorithm is as follows: \\
			Let $K$ be the kernel for the MH algo. At any point $X_0, Y_0$ of the chain, define the point-wise maxima as $\Pi_{X_0, Y_0}(\cdot) = \min(K(X_0, \cdot), K(Y_0, \cdot))$.\\
			Define the residuals as $r_{X_0}(\cdot) = K(X_0, \cdot) - \Pi_{X_0, Y_0}(\cdot)$ and $r_{Y_0}(\cdot) = K(Y_0, \cdot) - \Pi_{X_0, Y_0}(\cdot)$.\\
			Algorithm: \\
			w.p. $\Pi_{X_0, Y_0}(\mathbb{1})$, choose
				$$X_1 = Y_1 \sim \Pi_{X_0, Y_0}(\cdot)/\Pi_{X_0, Y_0}(\mathbb{1})$$
			Otherwise
				$$X_1 \sim r_{X_0}(\cdot)/r_{X_0}(\mathbb{1})$$
				$$Y_1 \sim r_{Y_0}(\cdot)/r_{Y_0}(\mathbb{1})$$
			Repeat the above steps to get the chain $(X_n)$ and $(Y_n)$

			\item Studied maximal coupling, optimal transport coupling, total variation norm, weisserstein distances.
			\item Meeting \#2 with Aaron:
				\begin{itemize}
					\item Discussed about when a coupling is faithful, coupling inequality and mixing time in our context.\\
					A coupling is said to be "good/faithful/stichy" if :
					$$\forall s>\Gamma_{\text{hit}}, X_s = Y_s,$$
					$$\text{where, } \Gamma_{\text{hit}}(x, y) = \min\{s: X_s = Y_s, X_0 = x, Y_0 = y\}$$
					\begin{theorem}
						$$||K^s(x, \cdot) - K^s(y, \cdot)||_{\text{TV}} \leq \Pr(\Gamma_{\text{hit}}(x, y) > s)$$
					\end{theorem}
					\begin{corollary}
						$$\Gamma_{\text{mix}}(\epsilon) \leq \min\{s : \max_{x, y} \Pr(\Gamma_{\text{hit}}(x, y) > s) \leq \epsilon\}$$
					\end{corollary}
					\item Discussed the main idea of Broderick et al's paper. The setting of the problem is basically we have a lot of vertices with +1 and -1 labellings.
					We want to connect them using edges to make a graph such that the energy of the graph is as minimum as possible (energy is lower if we minimize the number of edges between opposite signed vertices)
					This is kind of like a clustering problem. The problem is to draw from configurations like these.\\
					Energy of a particular configuration is of the form $H(\sigma) = -\sum_{(i, j) \in E} \sigma(i)\sigma(j)$, where $\sigma(i)$ denotes the label of the vertex of a particular configuration.\\
					The probability density is then of the form $p(\sigma) \propto \exp(-\beta H(\sigma))$ (doubt: what exactly are we drawing from this distribution, how to do it?)\\
					As there is no difference if we switch the labels in our case, we take the space to be $\overline{\Sigma} = \Sigma/(\text{flip})$
					\item The paper doesn't contribute much theoretically but is very well written. The work done is on a computer and some new things and ideas are presented. The aim of our project is to take one of the below presented problems, try to make some progress in them and write a Broderick style paper on it.
					\item We then discussed the following three problems:
						\begin{enumerate}
							\item NNMF : The problem is that we have a very large matrix and we need to decompose it into product of smaller dimension matrices such that the energy is minimized. \\
							$A \approx C^tR$, $H(C, R) \approx ||C|| + ||R|| + ||A-C^tR||$ (\todo: different norms are used, fill it up after reading the paper)\\
							Then draw from the defined distribution. $p(C, R) \propto \exp(-\beta H(C, R))$\\
							This is also similar to the classification problem, just not very obvious. (\todo: read the paper to understand why this problem is also similar to a classification problem)
							\item Ranking : Observe games $g_1(h_1, a_1) \dots g_N(h_N, a_N) \in \{-1, 1\}$, $h_i, a_i \in \{1, \dots, n\}/\Delta$, n: \#teams, N: \#games\\
							Based on the win/lose data, we try to rank them so that the energy is minimized. Energy is as follows: $H(\sigma) = \sum_{i = 1}^N \mathbb{1}_{\sigma(h_i)>\sigma{a_i}}\cdot\mathbb{1}_{g_i(h_i, a_i) = 1}$ (\todo: get a symmetric formula for energy (HINT: use the bradley-terry model)).\\
							Then draw from the probability distribution defined as $p(\sigma) \propto \exp(-\beta H(\sigma))$.\\
							This problem for $\beta = 0$ is called the random transposition walk (like glauber dynamics)
							\item Q: What happens when $\beta$ is small. Problem is a similar phenomenon to Gibbs algo on simplexes.
						\end{enumerate}
				\end{itemize}
			\item Studied convergence of Markov chains and application of coupling techniques to prove convergence.
		\end{itemize}

	\section{Week 3}
		Tasks for the week
		\begin{itemize}
			\item Complete Jacob's notes and lecture videos
			\item Broderick's paper
   			\item Study more about these problems, try to work out some details
		\end{itemize}
		Decided to work on NNMF

	\section{Week 4}
	\begin{itemize}
		\item Meeting \#3 with Aaron:
			\begin{itemize}
					\item Read the following stuff:
						\begin{itemize}
							\item hit and run as a unifying device (also see geyer notes on hit and run algorithms)
							\item "sandwich algorithm" by Kshitij Khare
							\item Hamiltonion Monte Carlo (simplest concrete example for our problem)
						\end{itemize}
					\item NNMF is analogous to mixture models, and we can visualize mixture model in the 1-dim case. Problem is first to find an analogous version of mixture models in the discrete case so that visualization is easier.
					\item Paper idea \#1 (easier): Deal with the label-switching problem in the case of $3$ modes by projecting state space.\\
					$M = [v_1, \cdots, v_1, v_2, \cdots, v_2, v_3, \cdots, v_3]$, modes are $C = [v_1, v_2]$ or $C = [v_1, v_3]$ or $C = [v_3, v_1]$, all possible label switchings, all convex combinations.\\
					These modes are good candidates in the case of $||\cdot||_0$, bad for $||\cdot||_{\infty}$. Need to test for all the other distances in between.\\
					The problem is to deal with all these label switching cases by projecting the state space to partition space of some sort as done in Broderick et. al's paper.
					\item Paper idea \#2 (tougher but more interesting): Work in the discrete state space setting, find couplings that hit in here, and try to tweak it so that the same thing kind of works in the continuous state as well.\\
					Our approach is the following:
					\begin{enumerate}
						\item From the general matrix $M'$ in consideration, we somehow get $M$ which has finite number of distinct columns less than the number of columns of $M'$ (this is a clustering problem on its own).
      					\item Now from columns of $M$, we get all the possible modes and consider a graph with those modes as our vertices.
           				\item Q: How does likelihood look on paths between nodes.
           				\item We find couplings in this graph that hits  in finite time (maybe we can use Broderick's ideas).
               			\item Then using the coupling in the discrete case, by some "tweaks" we can try to find a coupling in the continuous case.
					\end{enumerate}
					\item Another interesting question: in higher dimension, can we get situations where mixing is quick but meeting is slower? (for eg. in the case of random transposition walk, mixing is $O(n\log n)$ and meeting is $O(n^2)$)\\
					Idea: construct some examples where there are "good paths" between vertices of our graph.
			\end{itemize}
		\item Plan of action:
			First I'll start working on the easier idea.
			\begin{itemize}
				\item Hard code a 3 distinct vector matrix of choice.
    			\item Create a suitable penalty function and probability distribution using that function.
    			\item Samples nodes from this distribution using MCMC (MH-algo for starters).
			\end{itemize}
	\end{itemize}


\end{document}
